---
title: "final_project_dolgov"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Analysis of Faux Amis (False Cognates) translation quality in English-French Machine Translation


## Research objectives, hypotheses to be tested.

The main research objective of this study is finding the best machine translation model for English to French translation with an emphasis on false cognates. 

I have been interested in this topic for as long as I have been learning languages - the translation of so-called "faux amis" or "false friends". 
False cognates (or interlingual homonyms / paronyms) are words in two languages that are similar in spelling or pronunciation but different in meaning.

As language is an incredibly complex system that depends not only on its own internal rules but also on other languages, translation is a multifaceted and arduous endeavor. Despite their name, false friends do not help with this - on the contrary, they can lead to misunderstanding and incorrect translation of the text. 
In view of the fact that we are already seeing great development of machine translation systems, I believe it is important to study their ability to translate words that can easily mislead humans. 

I have chosen MBart, NLLB and ALIGN as the machine translation models that will be used to compare translation quality. All three models can translate from English to French and vice versa.


### Hypotheses.

I have highlighted the following null hypothesis: 

  H0: The translation quality is independent of the model.

It holds because modern models trained on high-quality bilingual datasets may well have sufficient context awareness abilities to translate sentences correctly in spite of false cognates.

There are three alternative hypotheses: 

  H1: MBart has a higher translation quality.
  
  H1: NLLB has a higher translation quality.
  
  H1: ALIGN has a higher translation quality.

One hypothesis is allocated to each of the models, in case one shows better quality than the others. This is quite realistic, because datasets can differ in quality, size, and representation of "false friends". 


## Previous research.

The topic of false cognates, especially in the French-English language pair, is relatively underdeveloped. 

The challenge of creating a dataset of interlingual paronyms in order to learn or improve machine translation models is addressed in the paper Challenge Dataset of Cognates and False Friend Pairs from Indian Languages [1], but, as is obvious from the title, only for Indian languages. 

In their paper Automatic Identification of Cognates, False Friends, and Partial Cognates [2], the author proposes a standalone tool for automatic identification of "false friends". The classifier was conceived to improve machine translation models and systems, and was designed for the French-English language pair.

However, the dataset that the authors used to train the classifier is not open to the public, and some of the references provided in the paper cannot be accessed. 


## Description of the original dataset. 

As there is no dataset focused specifically on false cognates in French and English in the public domain, I decided to create it myself. 
For this purpose, I defined a list of such word pairs and selected the most frequent ones (here the frequency is determined heuristically, based on many years of experience in learning both French and English on a high academic level). 
I parsed the specific pairs from several sites[3][4][5] listed at the end of the study, then hand-picked them. 
The final size of the dataset is 74 pairs of false cognates. 

For each pair, I wrote four sentences: 

- English sentence 

- French translation 

- French sentence 

- English translation

Then, I scored each translation using two metrics, the normalized Levenshtein distance and the BLEU score. 
Levenshtein distance gives a general sense of the similarity of two strings, and BLEU shows the quality of the translation specifically.

The variables in the dataset are divided into two parts:

1. Data for translation:

  - en_fa - "false friend" in English
  
  - fr_fa - "false friend" in French
  
  - mode - translation direction - EnFr | FrEn
  
  - sent - original message (language depends on translation direction)
  
  - trans - manually translated message 
  
  - trans_MBart - translation of the MBart model
  
  - trans_NLLB - translation of the NLLB model
  
  - trans_ALIGN - translation of the ALIGN model
  
  - POSent - position of the false cognate in the sentence - "start" | "mid" | "end"
  
2. Metrics for evaluating machine translation:

  - Levenshtein_MBart 
  
  - BLEU_MBart 
  
  - Levenshtein_NLLB 
  
  - BLEU_NLLB 
  
  - Levenshtein_ALIGN 
  
  - BLEU_ALIGN 

All variables in the first part are categorical (string), in the second part they are numeric, floating point variables. 

```{r}
library(tidyverse)

data <- read.csv("C:\\Users\\nikol\\OneDrive\\Desktop\\HSE_R\\FauxAmisR\\final_project_data_dolgov.csv")
summary(data)
head(data)
```

## Analysis Methods.

I will use the following methods to analyze the dataset: 

- ANOVA tests, T-test (statistical analysis)

- histograms, boxplots, corellogram (visualization)


## Descriptive Statistics.

```{r position}
library(ggplot2)

p <- data %>%
  ggplot(aes(x=POSent)) +
  geom_histogram(stat="count") +
  ggtitle('False cognate position distribution')
p
```

First, it is worth checking if there are empty elements in the dataset, or sentences that are much longer or shorter than others. 
Both of these criteria can affect the average quality of the translation.

```{r length}
print(paste("Number of empty elements:", sum(is.na(data))))

props <- prop.table(table(data$mode))[1]
print(paste("Proportion of English to French sentences: ", props))

data$len_sent <- nchar(data$sent)
data$len_trans <- nchar(data$trans)

print(paste("Mean original sentence length: ", mean(data$len_sent)))
print(paste("Mean original translation length: ", mean(data$len_trans)))

hist_data <- data %>%
  pivot_longer(cols=(len_sent:len_trans), names_to="attr", values_to="length")

p <- hist_data %>%
  ggplot(aes(x=length, group=attr, fill=attr)) +
  geom_histogram(binwidth=1, alpha=0.5, position="identity") +
  scale_x_continuous(name = "Length in symbols",
                           breaks=seq(0, 80, 5)) +
        scale_y_continuous(name = "Number of sentences",
                           breaks=seq(0, 15, 1)) +
  ggtitle("Sentence length") +
  geom_vline(xintercept=mean(data$len_sent), size=1, colour="blue", linetype="dashed") +
  geom_vline(xintercept=mean(data$len_trans), size=1, colour="red", linetype="dashed")
p
```

The dataset does not contain any empty sentences or other empty variables, sentences are represented by a normal distribution of length of both original and translated sentences. 
The average length value is approximately 41 symbols.

The ratio of sentences in both languages is exactly 50%. 

Next, it is worth analyzing the evaluation metrics. 

```{r metrics}
library(ggpubr)
box_data <- data %>%
  pivot_longer(cols=(c(Levenshtein_MBart, Levenshtein_NLLB, Levenshtein_ALIGN)), names_to="Levenshtein", values_to="value_lev") %>%
  pivot_longer(cols=(c(BLEU_MBart, BLEU_NLLB, BLEU_ALIGN)), names_to="BLEU", values_to="value_bleu")

lev_bp <- box_data %>%
 ggplot(aes(x=Levenshtein, y=value_lev, fill=Levenshtein)) +
 geom_boxplot() +
 labs(x = "", y = "") +
 theme_classic()

bleu_bp <- box_data %>%
 ggplot(aes(x=BLEU, y=value_bleu, fill=BLEU)) +
 geom_boxplot() +
 labs(x = "", y = "") +
 theme_classic()

arr <- ggarrange(lev_bp, bleu_bp, ncol=1, nrow=2)

annotate_figure(arr, top=text_grob("Metric distribution analysis"))

print(paste("Mean Levenshtein distance: ", mean(data$len_sent)))
```

```{r stats for metrics}
print("Statistical distribution of Levenshtein distance score")
print("MBart")
print(summary(data$Levenshtein_MBart))
print("NLLB")
print(summary(data$Levenshtein_NLLB))
print("ALIGN")
print(summary(data$Levenshtein_ALIGN))

print("Statistical distribution of BLEU score")
print("MBart")
print(summary(data$BLEU_MBart))
print("NLLB")
print(summary(data$BLEU_NLLB))
print("ALIGN")
print(summary(data$BLEU_ALIGN))
```

The presented statistics will be analyzed in the next section. 

It is also worthwhile to check the data for correlations, both for the correlation of metrics among themselves and for the correlation of metrics with other variables: length of original sentences, length of translations, direction of translation, and position of the false cognate in the sentence. 

```{r}
library(corrplot)

data_for_cor <- data %>%
  select(
    Levenshtein_MBart, Levenshtein_NLLB, Levenshtein_ALIGN, 
    BLEU_MBart, BLEU_NLLB, BLEU_ALIGN,
    len_sent, len_trans, 
    mode, POSent
    ) %>%
  mutate(mode = ifelse(mode == "EnFr", 1, 0)) %>%
  mutate(POSent = ifelse(POSent == "start", 0,
                         ifelse(POSent == "mid", 1, 2)))
print(data_for_cor)

cor_matrix <- cor(data_for_cor)

corrplot(cor_matrix, type = "upper",  
         method = "circle",
         addCoef.col = "black",  
         tl.col = "black", tl.srt = 45)
```

The correlogram shows a strong correlation between metrics of the same type, and a strong negative correlation between metrics of different types. 
As for the categorical variables, only one of them has a weak correlation with the BLEU metric of the MBart model. The others interact only with each other: the length of the original sentence correlates with the length of the translation and the length of the translation correlates with the direction of the translation.

The last two correlations can only support the fact that the models efficiently preserve text length in translation. 
It is also worth considering that one of the languages being compared, French, is known for its complex and verbose grammatical structures. Therefore, it can be assumed that the length of the sentence increases during English-to-French translation. 


## Quantitative Results.

Since the H0 for this study is that there is no significant difference in translation quality between the selected models, and there are three models, it makes sense to apply the ANOVA (analysis of variance). 
We need to understand whether the mean of the metrics of any of the models differs from the overall mean. 

In this one-way ANOVA test, the names of the metrics for the three models ("Levenshtein MBart", "Levenshtein NLLB", "Levenshtein ALIGN" and "BLEU MBart", "BLEU NLLB", "BLEU ALIGN") will be chosen as the independent variable from the Levenshtein and BLEU variables respectively. 

However, before conducting the ANOVA test, we need to make sure that the variance in the metrics for all three models is close to equal. To do this, we can perform a Levigne test. 

First, we will analyze the Levenshtein distance variables. 

```{r Levene Levenshtein}
library(car)

quantit_data <- box_data
quantit_data$Levenshtein <- factor(quantit_data$Levenshtein)

levene <- leveneTest(value_lev ~ Levenshtein, data=quantit_data)
levene
```

According to the results of the Levene test (high F-value and p-value < 0.05), it can be seen that the variance in the Levenshtein distance groups is different. 
This means that it is not possible to apply the one-way ANOVA - the homogeneity assumption is not met. 

In this case, it is reasonable to apply a nonparametric analog - the Kruskal-Wallis test. 

```{r Kruskal-Wallis}

kruskal.test(value_lev ~ Levenshtein, data=quantit_data)
```

The Kruskal-Wallis test showed a p-value much lower than 0.05, indicating a significant difference between the models.  

To understand exactly how the metrics of the models differ, we need to perform a post-hoc test. 
In this case, we need pairwise t-tests with correction for multiple comparisons, for example, the Wilcoxon test.

```{r post hoc Wilcoxon}
pairwise.wilcox.test(quantit_data$value_lev, quantit_data$Levenshtein, p.adj='BH')
```

The Wilcoxon test showed low (< 0.05) p-values for Levenshtein_MBart - Levenshtein_ALIGN and Levenshtein_MBart - Levenshtein_NLLB pairs. At the same time, the differences between Levenshtein NLLB and ALIGN are relatively insignificant. 

Based on the results of this test, we can conclude that there is a significant difference in the quality of the MBart model translation. 

Applying the analysis of the mean and median Levenshtein distances for this model, we see that MBart has the highest mean Levenshtein distance of 0.23 as opposed to 0.17 and 0.16 for NLLB and ALIGN, respectively. 

Since a high Levenshtein distance indicates low string similarity and thus low translation quality, we can draw the intermediate conclusion that MBart is inferior to NLLB and ALIGN, and of the latter pair, ALIGN has the highest translation quality. 

Now it is sensible to return to the second translation quality evaluation metric, BLEU. 
Repeating the procedure, we should make sure that the variance in the BLEU groups is approximately equal. 

```{r Levene BLEU}
quantit_data$BLEU <- factor(quantit_data$BLEU)
levene <- leveneTest(value_bleu ~ BLEU, data=quantit_data)
levene
```

Unlike the test results for the Levenshtein distance metric, the BLEU Levene test showed a relatively low F-value as well as a high p-value (>0.05). 
This means that the variance in the BLEU groups is not different, and a one-way ANOVA can be applied - the homogeneity assumption is met.

```{r one-way ANOVA}
anova_ow <- aov(value_bleu ~ BLEU, data=quantit_data)
summary(anova_ow)
```

The ANOVA test showed a relatively high F-value and low (< 0.05) p-value. This means that the null hypothesis, asserting that the models have equal translation quality, can be rejected, and we can proceed to post-hoc tests. 
Here, the Tukey test can be applied. 

```{r one-way Tukey}
tukey_ow <- TukeyHSD(anova_ow)
tukey_ow
```

Based on the Tukey test, we see low (< 0.05) p-values for the BLEU_MBart - BLEU_ALIGN and BLEU_NLLB - BLEU_MBart pairs, while the BLEU_NLLB - BLEU_ALIGN p-value is high. 
This again suggests that NLLB and ALIGN are roughly equal in quality, while MBart quality differs to a meaningful degree. 

Here is the average value of the BLEU metric for the models: 

- MBart: 0.43

- NLLB: 0.53

- ALIGN: 0.56

Combining the results of the statistical tests and the metrics, a clear advantage of ALIGN model translations is shown for the second time, with NLLB following and at the very end - MBart.


## Conclusion and discussion.

Summarizing the research work, a clear and concrete conclusion can be drawn about the model with the best quality translations based on the Levenshtein distance and the BLEU metric: ALIGN.  
Of the three selected models, this model has the highest (and lowest, in the case of the Levenshtein distance) average metrics. 

There are, however, improvements to be done that could enhance the quality of the analysis. 

1. Since the work is academic and was conducted under computationally limited conditions, models were chosen based on their size in parameters (75 million for ALIGN, 1.3 billion for NLLB, and 610 million for MBart). 
Translation quality tends to improve with larger models. 

2. The final dataset used 74 pairs of "false friends". While this dataset is sufficient to understand the overall picture of model translation quality, a larger sample of different cognates would certainly help in better understanding the final difference. 

3. Taking into account the two translation directions (French to English and English to French), the final dataset has 74 * 2 = 148 lines. As an improvement and continuation of the work, more lines can be added / collected / generated for the dataset. For example, instead of 4 sentences per pair of false cognates, 8-12 can be used. 

4. An interesting direction for the development of the work is to collect more metadata. The work analyzed the length of the original sentence, the translation, and the location of the cognates in the sentence. This list can be supplemented, for example, by the frequency of use of such words in communication in general. 

5. Finally, given the success of the ALIGN model that was trained on the "opus" dataset, it is possible to further fine-tune other models on the same dataset, modifying it to include sentences with "false friends".


## References.

  1. Challenge Dataset of Cognates and False Friend Pairs from Indian Languages, Diptesh Kanojia et al. [https://aclanthology.org/2020.lrec-1.378.pdf]
  [https://www.thoughtco.com/french-english-false-cognates-faux-amis-1364675]
  
  2. Automatic Identification of Cognates, False Friends, and Partial Cognates, Oana Magdalena Frunza et al. [https://www.researchgate.net/publication/277297339_Automatic_Identification_of_Cognates_False_Friends_and_Partial_Cognates]
  
Websites used for compiling the list of "false friends":

  3. [https://www.thoughtco.com/faux-amis-vocabulary-1371249]
  
  4. [https://www.vidalingua.com/blog/faux-amis-anglais]
  
  5. [https://wallstreetenglish.fr/fiches-anglais/general/faux-amis-anglais]
